{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b94e9df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\apaas\\anaconda3\\envs\\legal\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Building edges: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12747/12747 [00:01<00:00, 10558.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved PyTorch graph as global_graph.pt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# === Load everything ===\n",
    "df = pd.read_csv(\"graph.csv\")\n",
    "for col in ['facts', 'statutes', 'charges']:\n",
    "    df[col] = df[col].apply(ast.literal_eval)\n",
    "\n",
    "embeddings = np.load(\"node_embeddings_384.npy\")\n",
    "with open(\"node_index.json\") as f:\n",
    "    node_index = json.load(f)\n",
    "\n",
    "# === Create a global ID map ===\n",
    "id_map = {}\n",
    "counter = 0\n",
    "for t in ['case', 'fact', 'statute', 'charge']:\n",
    "    for node in node_index[t]:\n",
    "        id_map[node] = counter\n",
    "        counter += 1\n",
    "\n",
    "edges = []\n",
    "edge_types = []\n",
    "\n",
    "for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Building edges\"):\n",
    "    c = row['filename']\n",
    "    case_id = id_map[c]\n",
    "\n",
    "    # Connect case to facts\n",
    "    for f in row['facts']:\n",
    "        if f in id_map:\n",
    "            edges.append([case_id, id_map[f]])\n",
    "            edge_types.append('has_fact')\n",
    "\n",
    "    # Connect case to statutes\n",
    "    for s in row['statutes']:\n",
    "        if s in id_map:\n",
    "            edges.append([case_id, id_map[s]])\n",
    "            edge_types.append('refers_to')\n",
    "\n",
    "    # Connect case to charges\n",
    "    for ch in row['charges']:\n",
    "        if ch in id_map:\n",
    "            edges.append([case_id, id_map[ch]])\n",
    "            edge_types.append('charged_under')\n",
    "\n",
    "# Convert edges\n",
    "edge_index = torch.tensor(np.array(edges).T, dtype=torch.long)\n",
    "x = torch.tensor(embeddings, dtype=torch.float)\n",
    "\n",
    "# === Label Encoding ===\n",
    "y = torch.tensor(df['label'].values, dtype=torch.long)  # if label column exists per case\n",
    "\n",
    "# === Build PyG Graph ===\n",
    "data = Data(x=x, edge_index=edge_index, y=y)\n",
    "torch.save(data, \"global_graph.pt\")\n",
    "\n",
    "print(\"‚úÖ Saved PyTorch graph as global_graph.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3499480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Graph loaded: Data(x=[132695, 384], edge_index=[2, 169609], y=[12747])\n",
      "Node features: 132695, Labels: 12747\n",
      "Train nodes: 10197, Test nodes: 2550\n",
      "Detected 3 classes: [0, 1, 2]\n",
      "\n",
      "üöÄ Training started...\n",
      "\n",
      "Epoch 01 | Loss: 1.8666\n",
      "Epoch 05 | Loss: 1.2390\n",
      "Epoch 10 | Loss: 1.0717\n",
      "Epoch 15 | Loss: 0.9751\n",
      "Epoch 20 | Loss: 0.9191\n",
      "Epoch 25 | Loss: 0.8822\n",
      "Epoch 30 | Loss: 0.8545\n",
      "Epoch 35 | Loss: 0.8346\n",
      "Epoch 40 | Loss: 0.8165\n",
      "Epoch 45 | Loss: 0.7907\n",
      "Epoch 50 | Loss: 0.7751\n",
      "Epoch 55 | Loss: 0.7563\n",
      "Epoch 60 | Loss: 0.7356\n",
      "Epoch 65 | Loss: 0.7183\n",
      "Epoch 70 | Loss: 0.7089\n",
      "Epoch 75 | Loss: 0.7013\n",
      "Epoch 80 | Loss: 0.6853\n",
      "Epoch 85 | Loss: 0.6775\n",
      "Epoch 90 | Loss: 0.6702\n",
      "Epoch 95 | Loss: 0.6603\n",
      "Epoch 100 | Loss: 0.6590\n",
      "\n",
      "üìä Evaluation Metrics\n",
      "Accuracy  : 0.5024\n",
      "Precision : 0.3341\n",
      "Recall    : 0.3499\n",
      "F1-score  : 0.3315\n",
      "\n",
      "‚úÖ Model trained and saved as 'legal_gat_model.pt'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ==========================================\n",
    "# 1Ô∏è‚É£  Safe load of your saved graph\n",
    "# ==========================================\n",
    "torch.serialization.add_safe_globals([Data])  # allowlist PyG Data class\n",
    "\n",
    "data = torch.load(\"global_graph.pt\", weights_only=False)\n",
    "print(f\"‚úÖ Graph loaded: {data}\")\n",
    "\n",
    "# ==========================================\n",
    "# 2Ô∏è‚É£  Ensure labels & features match\n",
    "# ==========================================\n",
    "num_nodes = data.x.size(0)\n",
    "print(f\"Node features: {num_nodes}, Labels: {len(data.y)}\")\n",
    "\n",
    "# pad labels if unlabeled nodes exist\n",
    "if len(data.y) < num_nodes:\n",
    "    y_new = torch.full((num_nodes,), -1, dtype=torch.long)\n",
    "    y_new[:len(data.y)] = data.y\n",
    "    data.y = y_new\n",
    "\n",
    "# ==========================================\n",
    "# 3Ô∏è‚É£  Create train/test masks for labeled nodes\n",
    "# ==========================================\n",
    "labeled_mask = data.y >= 0\n",
    "labeled_indices = torch.nonzero(labeled_mask, as_tuple=False).squeeze()\n",
    "num_labeled = len(labeled_indices)\n",
    "train_size = int(0.8 * num_labeled)\n",
    "\n",
    "perm = torch.randperm(num_labeled)\n",
    "train_idx = labeled_indices[perm[:train_size]]\n",
    "test_idx = labeled_indices[perm[train_size:]]\n",
    "\n",
    "data.train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "data.test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "data.train_mask[train_idx] = True\n",
    "data.test_mask[test_idx] = True\n",
    "\n",
    "print(f\"Train nodes: {data.train_mask.sum().item()}, Test nodes: {data.test_mask.sum().item()}\")\n",
    "\n",
    "# ==========================================\n",
    "# 4Ô∏è‚É£  Detect number of classes automatically\n",
    "# ==========================================\n",
    "unique_labels = torch.unique(data.y[data.y >= 0])\n",
    "num_classes = int(unique_labels.max().item() + 1)\n",
    "print(f\"Detected {num_classes} classes:\", unique_labels.tolist())\n",
    "\n",
    "# ==========================================\n",
    "# 5Ô∏è‚É£  Define GAT model\n",
    "# ==========================================\n",
    "class LegalGAT(nn.Module):\n",
    "    def __init__(self, in_dim=384, hidden=128, out_dim=num_classes, heads=3):\n",
    "        super().__init__()\n",
    "        self.gat1 = GATConv(in_dim, hidden, heads=heads, dropout=0.6)\n",
    "        self.gat2 = GATConv(hidden * heads, out_dim, heads=1, concat=False, dropout=0.6)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.elu(self.gat1(x, edge_index))\n",
    "        x = self.gat2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "model = LegalGAT()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "\n",
    "# use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "data, model = data.to(device), model.to(device)\n",
    "\n",
    "# ==========================================\n",
    "# 6Ô∏è‚É£  Training loop\n",
    "# ==========================================\n",
    "print(\"\\nüöÄ Training started...\\n\")\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "        print(f\"Epoch {epoch+1:02d} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "# ==========================================\n",
    "# 7Ô∏è‚É£  Evaluation\n",
    "# ==========================================\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(data.x, data.edge_index)\n",
    "    preds = logits[data.test_mask].argmax(dim=1).cpu()\n",
    "    true = data.y[data.test_mask].cpu()\n",
    "\n",
    "acc  = accuracy_score(true, preds)\n",
    "prec = precision_score(true, preds, average='macro', zero_division=0)\n",
    "rec  = recall_score(true, preds, average='macro', zero_division=0)\n",
    "f1   = f1_score(true, preds, average='macro', zero_division=0)\n",
    "\n",
    "print(\"\\nüìä Evaluation Metrics\")\n",
    "print(f\"Accuracy  : {acc:.4f}\")\n",
    "print(f\"Precision : {prec:.4f}\")\n",
    "print(f\"Recall    : {rec:.4f}\")\n",
    "print(f\"F1-score  : {f1:.4f}\")\n",
    "\n",
    "# ==========================================\n",
    "# 8Ô∏è‚É£  Save model\n",
    "# ==========================================\n",
    "torch.save(model.state_dict(), \"legal_gat_model.pt\")\n",
    "print(\"\\n‚úÖ Model trained and saved as 'legal_gat_model.pt'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d2795f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'argmax'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n\u001b[32m      3\u001b[39m model.eval()\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m preds = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43margmax\u001b[49m(dim=\u001b[32m1\u001b[39m).cpu().numpy()\n\u001b[32m      5\u001b[39m true = data.y.cpu().numpy()\n\u001b[32m      7\u001b[39m acc = accuracy_score(true, preds)\n",
      "\u001b[31mAttributeError\u001b[39m: 'tuple' object has no attribute 'argmax'"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "model.eval()\n",
    "preds = model(data.x, data.edge_index).argmax(dim=1).cpu().numpy()\n",
    "true = data.y.cpu().numpy()\n",
    "\n",
    "acc = accuracy_score(true, preds)\n",
    "prec = precision_score(true, preds, average='macro')\n",
    "rec = recall_score(true, preds, average='macro')\n",
    "f1 = f1_score(true, preds, average='macro')\n",
    "\n",
    "print(f\"‚úÖ Accuracy : {acc:.4f}\")\n",
    "print(f\"‚úÖ Precision: {prec:.4f}\")\n",
    "print(f\"‚úÖ Recall   : {rec:.4f}\")\n",
    "print(f\"‚úÖ F1-score : {f1:.4f}\")\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(true, preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd3343d",
   "metadata": {},
   "outputs": [],
   "source": [
    "att_weights = model.gat1.att_src\n",
    "print(\"Attention Weights Shape:\", att_weights.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f47eda28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Graph loaded: Data(x=[132695, 384], edge_index=[2, 169609], y=[12747])\n",
      "Node features: 132695, Labels: 12747\n",
      "Charges vector:  torch.Size([12747, 9401])\n",
      "Statutes vector: torch.Size([12747, 27959])\n",
      "Train: 10197 | Test: 2550\n",
      "\n",
      "üöÄ Training Multi-Label GAT...\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:121] data. DefaultCPUAllocator: not enough memory: you tried to allocate 11367839616 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 100\u001b[39m\n\u001b[32m     98\u001b[39m model.train()\n\u001b[32m     99\u001b[39m optimizer.zero_grad()\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m out_c, out_s = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    102\u001b[39m loss_c = criterion(out_c[data.train_mask],  Y_charge[data.train_mask])\n\u001b[32m    103\u001b[39m loss_s = criterion(out_s[data.train_mask],  Y_statute[data.train_mask])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\apaas\\anaconda3\\envs\\legal\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\apaas\\anaconda3\\envs\\legal\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 78\u001b[39m, in \u001b[36mMultiHeadLegalGAT.forward\u001b[39m\u001b[34m(self, x, edge_index)\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, edge_index):\n\u001b[32m     77\u001b[39m     x = F.elu(\u001b[38;5;28mself\u001b[39m.gat1(x, edge_index))\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     out_charge  = torch.sigmoid(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgat2_charge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     79\u001b[39m     out_statute = torch.sigmoid(\u001b[38;5;28mself\u001b[39m.gat2_statute(x, edge_index))\n\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m out_charge, out_statute\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\apaas\\anaconda3\\envs\\legal\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\apaas\\anaconda3\\envs\\legal\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\apaas\\anaconda3\\envs\\legal\\Lib\\site-packages\\torch_geometric\\nn\\conv\\gat_conv.py:366\u001b[39m, in \u001b[36mGATConv.forward\u001b[39m\u001b[34m(self, x, edge_index, edge_attr, size, return_attention_weights)\u001b[39m\n\u001b[32m    362\u001b[39m alpha = \u001b[38;5;28mself\u001b[39m.edge_updater(edge_index, alpha=alpha, edge_attr=edge_attr,\n\u001b[32m    363\u001b[39m                           size=size)\n\u001b[32m    365\u001b[39m \u001b[38;5;66;03m# propagate_type: (x: OptPairTensor, alpha: Tensor)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m366\u001b[39m out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpropagate\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m=\u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m=\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    368\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.concat:\n\u001b[32m    369\u001b[39m     out = out.view(-\u001b[32m1\u001b[39m, \u001b[38;5;28mself\u001b[39m.heads * \u001b[38;5;28mself\u001b[39m.out_channels)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Temp\\torch_geometric.nn.conv.gat_conv_GATConv_propagate_wybbnae4.py:176\u001b[39m, in \u001b[36mpropagate\u001b[39m\u001b[34m(self, edge_index, x, alpha, size)\u001b[39m\n\u001b[32m    167\u001b[39m             kwargs = CollectArgs(\n\u001b[32m    168\u001b[39m                 x_j=hook_kwargs[\u001b[33m'\u001b[39m\u001b[33mx_j\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m    169\u001b[39m                 alpha=hook_kwargs[\u001b[33m'\u001b[39m\u001b[33malpha\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    172\u001b[39m                 dim_size=kwargs.dim_size,\n\u001b[32m    173\u001b[39m             )\n\u001b[32m    174\u001b[39m \u001b[38;5;66;03m# End Message Forward Pre Hook #########################################\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx_j\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mx_j\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m    \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[38;5;66;03m# Begin Message Forward Hook ###########################################\u001b[39;00m\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch.jit.is_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_compiling():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\apaas\\anaconda3\\envs\\legal\\Lib\\site-packages\\torch_geometric\\nn\\conv\\gat_conv.py:414\u001b[39m, in \u001b[36mGATConv.message\u001b[39m\u001b[34m(self, x_j, alpha)\u001b[39m\n\u001b[32m    413\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmessage\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_j: Tensor, alpha: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m414\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malpha\u001b[49m\u001b[43m.\u001b[49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_j\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: [enforce fail at alloc_cpu.cpp:121] data. DefaultCPUAllocator: not enough memory: you tried to allocate 11367839616 bytes."
     ]
    }
   ],
   "source": [
    "# ===========================================================\n",
    "# Smart Legal Judgment Prediction ‚Äì Multi-Label GAT\n",
    "# Predicts Charges & Statutes based on Case Facts\n",
    "# ===========================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score, hamming_loss, precision_score, recall_score\n",
    "import pandas as pd, ast, warnings, numpy as np\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ==========================================\n",
    "# 1Ô∏è‚É£  Load your pre-built graph\n",
    "# ==========================================\n",
    "torch.serialization.add_safe_globals([Data])\n",
    "data = torch.load(\"global_graph.pt\", weights_only=False)\n",
    "print(f\"‚úÖ Graph loaded: {data}\")\n",
    "\n",
    "num_nodes = data.x.size(0)\n",
    "print(f\"Node features: {num_nodes}, Labels: {len(data.y)}\")\n",
    "\n",
    "# Pad labels (for completeness)\n",
    "if len(data.y) < num_nodes:\n",
    "    y_new = torch.full((num_nodes,), -1, dtype=torch.long)\n",
    "    y_new[:len(data.y)] = data.y\n",
    "    data.y = y_new\n",
    "\n",
    "# ==========================================\n",
    "# 2Ô∏è‚É£  Load CSV for multi-label targets\n",
    "# ==========================================\n",
    "df = pd.read_csv(\"graph.csv\")\n",
    "df[\"charges\"]  = df[\"charges\"].apply(ast.literal_eval)\n",
    "df[\"statutes\"] = df[\"statutes\"].apply(ast.literal_eval)\n",
    "\n",
    "charge_bin  = MultiLabelBinarizer()\n",
    "statute_bin = MultiLabelBinarizer()\n",
    "Y_charge  = torch.tensor(charge_bin.fit_transform(df[\"charges\"]),  dtype=torch.float)\n",
    "Y_statute = torch.tensor(statute_bin.fit_transform(df[\"statutes\"]), dtype=torch.float)\n",
    "\n",
    "print(f\"Charges vector:  {Y_charge.shape}\")\n",
    "print(f\"Statutes vector: {Y_statute.shape}\")\n",
    "\n",
    "# ==========================================\n",
    "# 3Ô∏è‚É£  Train/Test masks (case-level)\n",
    "# ==========================================\n",
    "num_cases = Y_charge.shape[0]\n",
    "perm = torch.randperm(num_cases)\n",
    "train_size = int(0.8 * num_cases)\n",
    "train_idx = perm[:train_size]\n",
    "test_idx  = perm[train_size:]\n",
    "\n",
    "data.train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "data.test_mask  = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "data.train_mask[train_idx] = True\n",
    "data.test_mask[test_idx]  = True\n",
    "\n",
    "print(f\"Train: {data.train_mask.sum().item()} | Test: {data.test_mask.sum().item()}\")\n",
    "\n",
    "# ==========================================\n",
    "# 4Ô∏è‚É£  Define multi-head LegalGAT\n",
    "# ==========================================\n",
    "class MultiHeadLegalGAT(nn.Module):\n",
    "    def __init__(self, in_dim=384, hidden=128,\n",
    "                 out_charges=Y_charge.shape[1],\n",
    "                 out_statutes=Y_statute.shape[1],\n",
    "                 heads=3):\n",
    "        super().__init__()\n",
    "        self.gat1 = GATConv(in_dim, hidden, heads=heads, dropout=0.5)\n",
    "        self.gat2_charge  = GATConv(hidden * heads, out_charges, heads=1, concat=False, dropout=0.5)\n",
    "        self.gat2_statute = GATConv(hidden * heads, out_statutes, heads=1, concat=False, dropout=0.5)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.elu(self.gat1(x, edge_index))\n",
    "        out_charge  = torch.sigmoid(self.gat2_charge(x, edge_index))\n",
    "        out_statute = torch.sigmoid(self.gat2_statute(x, edge_index))\n",
    "        return out_charge, out_statute\n",
    "\n",
    "# ==========================================\n",
    "# 5Ô∏è‚É£  Setup model, optimizer, device\n",
    "# ==========================================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MultiHeadLegalGAT().to(device)\n",
    "data  = data.to(device)\n",
    "Y_charge, Y_statute = Y_charge.to(device), Y_statute.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.003, weight_decay=5e-4)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# ==========================================\n",
    "# 6Ô∏è‚É£  Training loop\n",
    "# ==========================================\n",
    "print(\"\\nüöÄ Training Multi-Label GAT...\\n\")\n",
    "for epoch in range(40):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out_c, out_s = model(data.x, data.edge_index)\n",
    "\n",
    "    loss_c = criterion(out_c[data.train_mask],  Y_charge[data.train_mask])\n",
    "    loss_s = criterion(out_s[data.train_mask],  Y_statute[data.train_mask])\n",
    "    loss = loss_c + loss_s\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "        print(f\"Epoch {epoch+1:02d} | Total Loss: {loss.item():.4f} \"\n",
    "              f\"(Charges: {loss_c.item():.4f}, Statutes: {loss_s.item():.4f})\")\n",
    "\n",
    "# ==========================================\n",
    "# 7Ô∏è‚É£  Evaluation\n",
    "# ==========================================\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pred_c, pred_s = model(data.x, data.edge_index)\n",
    "    pred_c = (pred_c[data.test_mask] > 0.5).float().cpu().numpy()\n",
    "    pred_s = (pred_s[data.test_mask] > 0.5).float().cpu().numpy()\n",
    "    true_c = Y_charge[data.test_mask].cpu().numpy()\n",
    "    true_s = Y_statute[data.test_mask].cpu().numpy()\n",
    "\n",
    "def evaluate_multilabel(true, pred, name):\n",
    "    ham = hamming_loss(true, pred)\n",
    "    micro_f1 = f1_score(true, pred, average='micro', zero_division=0)\n",
    "    macro_f1 = f1_score(true, pred, average='macro', zero_division=0)\n",
    "    prec = precision_score(true, pred, average='micro', zero_division=0)\n",
    "    rec  = recall_score(true, pred, average='micro', zero_division=0)\n",
    "\n",
    "    print(f\"\\nüìä {name} Prediction Results\")\n",
    "    print(f\"Hamming Loss : {ham:.4f}\")\n",
    "    print(f\"Precision    : {prec:.4f}\")\n",
    "    print(f\"Recall       : {rec:.4f}\")\n",
    "    print(f\"Micro-F1     : {micro_f1:.4f}\")\n",
    "    print(f\"Macro-F1     : {macro_f1:.4f}\")\n",
    "\n",
    "evaluate_multilabel(true_c, pred_c, \"Charge\")\n",
    "evaluate_multilabel(true_s, pred_s, \"Statute\")\n",
    "\n",
    "# ==========================================\n",
    "# 8Ô∏è‚É£  Save model\n",
    "# ==========================================\n",
    "torch.save(model.state_dict(), \"multi_label_legal_gat.pt\")\n",
    "print(\"\\n‚úÖ Model trained & saved as 'multi_label_legal_gat.pt'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bb2af92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Graph loaded with 132695 nodes and 169609 edges.\n",
      "Charges vector:  torch.Size([12747, 300])\n",
      "Statutes vector: torch.Size([12747, 300])\n",
      "Train cases: 10197 | Test cases: 2550\n",
      "\n",
      "üîç charge label density: 0.003923\n",
      "üîç statute label density: 0.004660\n",
      "\n",
      "üöÄ Training (Weighted BCE mode)...\n",
      "\n",
      "Epoch 01 | Avg Loss: 3.4298\n",
      "Epoch 05 | Avg Loss: 2.5896\n",
      "Epoch 10 | Avg Loss: 2.3077\n",
      "Epoch 15 | Avg Loss: 2.2184\n",
      "Epoch 20 | Avg Loss: 2.1551\n",
      "Epoch 25 | Avg Loss: 2.1208\n",
      "Epoch 30 | Avg Loss: 2.1038\n",
      "Epoch 35 | Avg Loss: 2.0913\n",
      "Epoch 40 | Avg Loss: 2.0760\n",
      "Epoch 45 | Avg Loss: 2.0709\n",
      "Epoch 50 | Avg Loss: 2.0679\n",
      "Epoch 55 | Avg Loss: 2.0677\n",
      "Epoch 60 | Avg Loss: 2.0505\n",
      "Epoch 65 | Avg Loss: 2.0572\n",
      "Epoch 70 | Avg Loss: 2.0552\n",
      "Epoch 75 | Avg Loss: 2.0496\n",
      "Epoch 80 | Avg Loss: 2.0457\n",
      "Epoch 85 | Avg Loss: 2.0492\n",
      "Epoch 90 | Avg Loss: 2.0379\n",
      "Epoch 95 | Avg Loss: 2.0438\n",
      "Epoch 100 | Avg Loss: 2.0445\n",
      "Epoch 105 | Avg Loss: 2.0381\n",
      "Epoch 110 | Avg Loss: 2.0441\n",
      "Epoch 115 | Avg Loss: 2.0377\n",
      "Epoch 120 | Avg Loss: 2.0370\n",
      "Epoch 125 | Avg Loss: 2.0354\n",
      "Epoch 130 | Avg Loss: 2.0260\n",
      "Epoch 135 | Avg Loss: 2.0270\n",
      "Epoch 140 | Avg Loss: 2.0348\n",
      "Epoch 145 | Avg Loss: 2.0309\n",
      "Epoch 150 | Avg Loss: 2.0275\n",
      "Epoch 155 | Avg Loss: 2.0205\n",
      "Epoch 160 | Avg Loss: 2.0287\n",
      "Epoch 165 | Avg Loss: 2.0341\n",
      "Epoch 170 | Avg Loss: 2.0242\n",
      "Epoch 175 | Avg Loss: 2.0225\n",
      "Epoch 180 | Avg Loss: 2.0249\n",
      "Epoch 185 | Avg Loss: 2.0181\n",
      "Epoch 190 | Avg Loss: 2.0222\n",
      "Epoch 195 | Avg Loss: 2.0246\n",
      "Epoch 200 | Avg Loss: 2.0126\n",
      "Epoch 205 | Avg Loss: 2.0235\n",
      "Epoch 210 | Avg Loss: 2.0170\n",
      "Epoch 215 | Avg Loss: 2.0241\n",
      "Epoch 220 | Avg Loss: 2.0205\n",
      "Epoch 225 | Avg Loss: 2.0190\n",
      "Epoch 230 | Avg Loss: 2.0201\n",
      "Epoch 235 | Avg Loss: 2.0174\n",
      "Epoch 240 | Avg Loss: 2.0002\n",
      "Epoch 245 | Avg Loss: 2.0117\n",
      "Epoch 250 | Avg Loss: 2.0197\n",
      "Epoch 255 | Avg Loss: 2.0217\n",
      "Epoch 260 | Avg Loss: 2.0203\n",
      "Epoch 265 | Avg Loss: 2.0200\n",
      "Epoch 270 | Avg Loss: 2.0151\n",
      "Epoch 275 | Avg Loss: 2.0085\n",
      "Epoch 280 | Avg Loss: 2.0184\n",
      "Epoch 285 | Avg Loss: 2.0140\n",
      "Epoch 290 | Avg Loss: 2.0141\n",
      "Epoch 295 | Avg Loss: 2.0100\n",
      "Epoch 300 | Avg Loss: 2.0078\n",
      "Epoch 305 | Avg Loss: 2.0105\n",
      "Epoch 310 | Avg Loss: 2.0159\n",
      "Epoch 315 | Avg Loss: 2.0084\n",
      "Epoch 320 | Avg Loss: 2.0090\n",
      "Epoch 325 | Avg Loss: 2.0104\n",
      "Epoch 330 | Avg Loss: 2.0110\n",
      "Epoch 335 | Avg Loss: 2.0072\n",
      "Epoch 340 | Avg Loss: 2.0168\n",
      "Epoch 345 | Avg Loss: 2.0129\n",
      "Epoch 350 | Avg Loss: 2.0082\n",
      "Epoch 355 | Avg Loss: 2.0114\n",
      "Epoch 360 | Avg Loss: 2.0188\n",
      "Epoch 365 | Avg Loss: 2.0131\n",
      "Epoch 370 | Avg Loss: 2.0011\n",
      "Epoch 375 | Avg Loss: 2.0145\n",
      "Epoch 380 | Avg Loss: 2.0210\n",
      "Epoch 385 | Avg Loss: 2.0152\n",
      "Epoch 390 | Avg Loss: 2.0074\n",
      "Epoch 395 | Avg Loss: 2.0167\n",
      "Epoch 400 | Avg Loss: 1.9967\n",
      "Epoch 405 | Avg Loss: 1.9995\n",
      "Epoch 410 | Avg Loss: 2.0096\n",
      "Epoch 415 | Avg Loss: 2.0004\n",
      "Epoch 420 | Avg Loss: 1.9978\n",
      "Epoch 425 | Avg Loss: 2.0075\n",
      "Epoch 430 | Avg Loss: 2.0040\n",
      "Epoch 435 | Avg Loss: 2.0001\n",
      "Epoch 440 | Avg Loss: 2.0115\n",
      "Epoch 445 | Avg Loss: 2.0088\n",
      "Epoch 450 | Avg Loss: 1.9997\n",
      "Epoch 455 | Avg Loss: 2.0047\n",
      "Epoch 460 | Avg Loss: 2.0092\n",
      "Epoch 465 | Avg Loss: 2.0014\n",
      "Epoch 470 | Avg Loss: 2.0040\n",
      "Epoch 475 | Avg Loss: 2.0015\n",
      "Epoch 480 | Avg Loss: 2.0114\n",
      "Epoch 485 | Avg Loss: 2.0077\n",
      "Epoch 490 | Avg Loss: 2.0040\n",
      "Epoch 495 | Avg Loss: 2.0023\n",
      "Epoch 500 | Avg Loss: 2.0162\n",
      "\n",
      "üìä Charge Prediction Results @thr=0.3\n",
      "Hamming Loss : 0.4231\n",
      "Precision    : 0.0097\n",
      "Recall       : 0.9994\n",
      "Micro-F1     : 0.0193\n",
      "Macro-F1     : 0.0155\n",
      "\n",
      "üìä Statute Prediction Results @thr=0.3\n",
      "Hamming Loss : 0.4995\n",
      "Precision    : 0.0099\n",
      "Recall       : 0.9979\n",
      "Micro-F1     : 0.0197\n",
      "Macro-F1     : 0.0163\n",
      "\n",
      "üìä Charge Prediction Results @thr=0.2\n",
      "Hamming Loss : 0.6600\n",
      "Precision    : 0.0063\n",
      "Recall       : 1.0000\n",
      "Micro-F1     : 0.0124\n",
      "Macro-F1     : 0.0099\n",
      "\n",
      "üìä Statute Prediction Results @thr=0.2\n",
      "Hamming Loss : 0.7438\n",
      "Precision    : 0.0067\n",
      "Recall       : 1.0000\n",
      "Micro-F1     : 0.0133\n",
      "Macro-F1     : 0.0116\n",
      "\n",
      "üìä Charge Prediction Results @thr=0.1\n",
      "Hamming Loss : 0.9278\n",
      "Precision    : 0.0045\n",
      "Recall       : 1.0000\n",
      "Micro-F1     : 0.0089\n",
      "Macro-F1     : 0.0084\n",
      "\n",
      "üìä Statute Prediction Results @thr=0.1\n",
      "Hamming Loss : 0.9550\n",
      "Precision    : 0.0052\n",
      "Recall       : 1.0000\n",
      "Micro-F1     : 0.0104\n",
      "Macro-F1     : 0.0101\n",
      "\n",
      "‚úÖ Model trained & saved as 'legal_gat_weighted.pt'\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# ‚öñÔ∏è Smart Legal Judgment Prediction ‚Äì CPU-Friendly Weighted GAT\n",
    "# Multi-Label (Charges + Statutes)\n",
    "# ===============================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score, hamming_loss, precision_score, recall_score\n",
    "import pandas as pd, ast, numpy as np, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ===============================================================\n",
    "# 1Ô∏è‚É£ Load Graph\n",
    "# ===============================================================\n",
    "torch.serialization.add_safe_globals([Data])\n",
    "data = torch.load(\"global_graph.pt\", weights_only=False)\n",
    "print(f\"‚úÖ Graph loaded with {data.num_nodes} nodes and {data.num_edges} edges.\")\n",
    "\n",
    "num_nodes = data.x.size(0)\n",
    "if not hasattr(data, \"y\") or data.y is None:\n",
    "    data.y = torch.zeros(num_nodes, dtype=torch.long)\n",
    "elif len(data.y) < num_nodes:\n",
    "    y_new = torch.full((num_nodes,), -1, dtype=torch.long)\n",
    "    y_new[:len(data.y)] = data.y\n",
    "    data.y = y_new\n",
    "\n",
    "# ===============================================================\n",
    "# 2Ô∏è‚É£ Load and Preprocess CSV\n",
    "# ===============================================================\n",
    "df = pd.read_csv(\"graph.csv\")\n",
    "df[\"charges\"]  = df[\"charges\"].apply(ast.literal_eval)\n",
    "df[\"statutes\"] = df[\"statutes\"].apply(ast.literal_eval)\n",
    "\n",
    "# keep only top frequent labels\n",
    "top_charges  = df[\"charges\"].explode().value_counts().head(300).index\n",
    "top_statutes = df[\"statutes\"].explode().value_counts().head(300).index\n",
    "df[\"charges\"]  = df[\"charges\"].apply(lambda x: [c for c in x if c in top_charges])\n",
    "df[\"statutes\"] = df[\"statutes\"].apply(lambda x: [s for s in x if s in top_statutes])\n",
    "\n",
    "charge_bin  = MultiLabelBinarizer()\n",
    "statute_bin = MultiLabelBinarizer()\n",
    "Y_charge  = torch.tensor(charge_bin.fit_transform(df[\"charges\"]),  dtype=torch.float)\n",
    "Y_statute = torch.tensor(statute_bin.fit_transform(df[\"statutes\"]), dtype=torch.float)\n",
    "\n",
    "print(f\"Charges vector:  {Y_charge.shape}\")\n",
    "print(f\"Statutes vector: {Y_statute.shape}\")\n",
    "\n",
    "# ===============================================================\n",
    "# 3Ô∏è‚É£ Node ‚Üî Case Index Mapping\n",
    "# ===============================================================\n",
    "case_count = len(df)\n",
    "case_indices = torch.arange(case_count)  # assume first N nodes = cases\n",
    "\n",
    "# train/test split\n",
    "perm = torch.randperm(case_count)\n",
    "train_size = int(0.8 * case_count)\n",
    "train_idx = perm[:train_size]\n",
    "test_idx  = perm[train_size:]\n",
    "\n",
    "data.train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "data.test_mask  = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "data.train_mask[case_indices[train_idx]] = True\n",
    "data.test_mask[case_indices[test_idx]]   = True\n",
    "\n",
    "print(f\"Train cases: {len(train_idx)} | Test cases: {len(test_idx)}\")\n",
    "\n",
    "# ===============================================================\n",
    "# 4Ô∏è‚É£ Lightweight Multi-Head GAT\n",
    "# ===============================================================\n",
    "class MultiHeadLegalGAT(nn.Module):\n",
    "    def __init__(self, in_dim=384, hidden=64,\n",
    "                 out_charges=Y_charge.shape[1],\n",
    "                 out_statutes=Y_statute.shape[1],\n",
    "                 heads=2):\n",
    "        super().__init__()\n",
    "        self.gat1 = GATConv(in_dim, hidden, heads=heads, dropout=0.6)\n",
    "        self.gat2_charge  = GATConv(hidden * heads, out_charges, heads=1, concat=False, dropout=0.6)\n",
    "        self.gat2_statute = GATConv(hidden * heads, out_statutes, heads=1, concat=False, dropout=0.6)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.elu(self.gat1(x, edge_index))\n",
    "        out_c = self.gat2_charge(x, edge_index)\n",
    "        out_s = self.gat2_statute(x, edge_index)\n",
    "        return out_c, out_s  # logits (no sigmoid!)\n",
    "\n",
    "# ===============================================================\n",
    "# 5Ô∏è‚É£ Setup & Weighted Loss\n",
    "# ===============================================================\n",
    "device = torch.device(\"cpu\")\n",
    "model = MultiHeadLegalGAT().to(device)\n",
    "data  = data.to(device)\n",
    "Y_charge, Y_statute = Y_charge.to(device), Y_statute.to(device)\n",
    "\n",
    "# class imbalance weights\n",
    "pos_weight_c = (Y_charge.numel() - Y_charge.sum()) / (Y_charge.sum() + 1e-8)\n",
    "pos_weight_s = (Y_statute.numel() - Y_statute.sum()) / (Y_statute.sum() + 1e-8)\n",
    "criterion_c = nn.BCEWithLogitsLoss(pos_weight=pos_weight_c)\n",
    "criterion_s = nn.BCEWithLogitsLoss(pos_weight=pos_weight_s)\n",
    "sigmoid = nn.Sigmoid()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.003, weight_decay=5e-4)\n",
    "\n",
    "# print density for sanity check\n",
    "print(f\"\\nüîç charge label density: {Y_charge.mean().item():.6f}\")\n",
    "print(f\"üîç statute label density: {Y_statute.mean().item():.6f}\\n\")\n",
    "\n",
    "# ===============================================================\n",
    "# 6Ô∏è‚É£ Training Loop (mini-batch, CPU-friendly)\n",
    "# ===============================================================\n",
    "print(\"üöÄ Training (Weighted BCE mode)...\\n\")\n",
    "batch_size = 512\n",
    "indices = torch.arange(case_count)\n",
    "\n",
    "for epoch in range(500):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for i in range(0, len(indices), batch_size):\n",
    "        batch_idx = indices[i:i+batch_size]\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits_c, logits_s = model(data.x, data.edge_index)\n",
    "        logits_c, logits_s = logits_c[case_indices], logits_s[case_indices]\n",
    "\n",
    "        loss_c = criterion_c(logits_c[batch_idx], Y_charge[batch_idx])\n",
    "        loss_s = criterion_s(logits_s[batch_idx], Y_statute[batch_idx])\n",
    "        loss = loss_c + loss_s\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "        avg_loss = total_loss / (len(indices) // batch_size + 1)\n",
    "        print(f\"Epoch {epoch+1:02d} | Avg Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# ===============================================================\n",
    "# 7Ô∏è‚É£ Evaluation\n",
    "# ===============================================================\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits_c, logits_s = model(data.x, data.edge_index)\n",
    "    prob_c, prob_s = sigmoid(logits_c[case_indices]), sigmoid(logits_s[case_indices])\n",
    "\n",
    "true_c, true_s = Y_charge.cpu().numpy(), Y_statute.cpu().numpy()\n",
    "prob_c, prob_s = prob_c.cpu().numpy(), prob_s.cpu().numpy()\n",
    "\n",
    "def evaluate_multilabel(true, prob, name, threshold=0.2):\n",
    "    pred = (prob > threshold).astype(int)\n",
    "    ham = hamming_loss(true, pred)\n",
    "    micro_f1 = f1_score(true, pred, average='micro', zero_division=0)\n",
    "    macro_f1 = f1_score(true, pred, average='macro', zero_division=0)\n",
    "    prec = precision_score(true, pred, average='micro', zero_division=0)\n",
    "    rec  = recall_score(true, pred, average='micro', zero_division=0)\n",
    "    print(f\"\\nüìä {name} Prediction Results @thr={threshold}\")\n",
    "    print(f\"Hamming Loss : {ham:.4f}\")\n",
    "    print(f\"Precision    : {prec:.4f}\")\n",
    "    print(f\"Recall       : {rec:.4f}\")\n",
    "    print(f\"Micro-F1     : {micro_f1:.4f}\")\n",
    "    print(f\"Macro-F1     : {macro_f1:.4f}\")\n",
    "\n",
    "for thr in [0.3, 0.2, 0.1]:\n",
    "    evaluate_multilabel(true_c[test_idx], prob_c[test_idx], \"Charge\", threshold=thr)\n",
    "    evaluate_multilabel(true_s[test_idx], prob_s[test_idx], \"Statute\", threshold=thr)\n",
    "\n",
    "# ===============================================================\n",
    "# 8Ô∏è‚É£ Save Model\n",
    "# ===============================================================\n",
    "torch.save(model.state_dict(), \"legal_gat_weighted.pt\")\n",
    "print(\"\\n‚úÖ Model trained & saved as 'legal_gat_weighted.pt'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a858ebab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "charge label density: 0.00392327606495646\n",
      "statute label density: 0.004660442980047593\n",
      "0.04717245325446129 0.18653100728988647\n",
      "pred positives charge: 0.0\n",
      "pred positives statute: 0.0\n"
     ]
    }
   ],
   "source": [
    "# 1Ô∏è‚É£ are there any positive labels at all?\n",
    "print(\"charge label density:\", Y_charge.sum().item() / Y_charge.numel())\n",
    "print(\"statute label density:\", Y_statute.sum().item() / Y_statute.numel())\n",
    "\n",
    "# 2Ô∏è‚É£ how confident are predictions?\n",
    "out_c, out_s = model(data.x, data.edge_index)\n",
    "print(out_c.min().item(), out_c.max().item())\n",
    "\n",
    "# 3Ô∏è‚É£ fraction of positives predicted\n",
    "print(\"pred positives charge:\", (out_c > 0.3).float().mean().item())\n",
    "print(\"pred positives statute:\", (out_s > 0.3).float().mean().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7ddda5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "legal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
